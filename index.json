[{"content":"introduction I\u0026rsquo;ve written a few integration test frameworks where the tests were written as JUnit @Test methods and defined via a fluent API that hid the plumbing from the test author. These were always executed against a mixture of locally installed \u0026amp; in-memory infrastructure.\nThis worked well-enough in small development teams where the developers wrote the tests but wasn\u0026rsquo;t so good when I considered it for a larger project with a bigger team. On this project:\n Less technical team-members needed to be able to write tests. The infrastructure was more complex \u0026amp; the team larger so there was a greater overhead of installing \u0026amp; maintaining the required software on each machine. The system-under-test included multiple applications rather than multiple components of a single larger application. These components might be using different versions of the same libraries so could not share a test-time classpath.  I found an alternative approach using Cucumber \u0026amp; TestContainers that addressed these challenges. This is not intended to be a comprehensive introduction to either Cucumber or Testcontainers - we\u0026rsquo;ll mostly be concerned with how to use these two tools together to write BDD-style integration tests.\ncucumber A BDD framework seemed a good fit for enabling non-developers to write tests and had the added advantage of replacing separate acceptance criteria in stories. I looked at JBehave \u0026amp; Cucumber \u0026amp; liked the look of the latter.\nCucumber tests are written as features using a business-readable language called Gherkin. It has just a handful of keywords used to define the preconditions, stimulus \u0026amp; expected outcome of a test in a Given-When-Then form. The implementation of these steps are then defined programmatically.\nFor example, given the following scenario defined using Gherkin\u0026hellip;\nScenario:No coat required when it\u0026#39;s warm Given The temperature is 20degrees. When I ask whether I need a coat Then I should be told \u0026#34;No\u0026#34; \u0026hellip;the implementation of the first step would be defined in a method annotated like this.1\n@Given(\u0026#34;The temperature is 20 degrees.\u0026#34;) public void temperatureIsTwentyDegrees() { // TODO interact with the system-under-test to set the temperature } testcontainers To address the other challenges, I decided to use Testcontainers - a rather amazing library for starting infrastructure like databases or messaging systems in Docker containers at test-time. You just need Docker installed to use it.\nFor example, starting a Maria DB instance in a JUnit test might look like this:\n@Container private MariaDBContainer database = new MariaDBContainer( DockerImageName.parse(\u0026#34;mariadb\u0026#34;).withTag(\u0026#34;10.5.9\u0026#34;) ); Every developer/build machine then gets exactly the same versions of infrastructure at test-time without any local installation (other than Docker itself). Upgrading that infrastructure simply involves changing the image used in the test.\nWhen used for databases, this approach also prevents contamination of the database between test runs.\nBy generating docker images from application builds too, I was also able to use Testcontainers to start the applications under test avoiding issues with conflicting library versions.2\nsystem under test (SUT) My real SUT involved multiple kafka-steams components and topics but here we\u0026rsquo;ll use a simplified example with a single kafka-stream sufficient to demonstrate the approach. The stream will receive details of placed orders from one topic, enrich them with details of the customer \u0026amp; product(s) from a database and provide a summary of the sale on another topic.\nAll messages will be in json.\nYou can see the source for the system under test here.\nIf this were a real application we might integrate the production of the docker image into the maven build but, for simplicity, here we\u0026rsquo;ll use an external Dockerfile. The image can be built by running the command docker build -t order-enrichment-stream . after running the maven build.\ntest framework To test this system we\u0026rsquo;ll start three containers - one each for Kafka, Maria DB \u0026amp; our Kafka Streams Application.\nWe\u0026rsquo;ll go over the layout of the project and then go into detail on each of the key components.\nproject set-up The project structure will look like this:\n. ├── pom.xml └── src/test ├── java │ └── dev.codecrumbs.sales.test │ │ ├──Pipeline.java │ │ ├──StepDefinitions.java │ │ ├──Topic.java │ │ └──Utils.java │ └── features.IntegrationTests.java └── resources ├── features │ └── sales-summarised.feature ├── test-data │ └── ... ├── cucumber.properties ├── junit-platform.properties └── schema.sql The key parts of this are:\nOur Feature - sales-summarised.feature (\u0026amp; the supporting json files under test-data) that defines the expected behaviour.\nThe Step Definitions - cucumber-annotated implementations of the test steps used in our feature.\nThe Pipeline class - an abstraction of our Kafka-pipeline that allows us to interact with the pipeline from our step definitions and controls start-up of the required containers.\nMaven \u0026amp; CLI Configuration - IntegrationTests.java, cucumber.properties \u0026amp; junit-platform.properties that allow the tests to be run from our IDE \u0026amp; Maven.\nour feature Our feature file will look like this. Note that we are using Cucumber\u0026rsquo;s support for parameterised test steps to allow us to run the same test multiple times with different inputs \u0026amp; expected outputs.\nFeature:Sales are ingested. Scenario:Details of a sale are ingested. Scenario Outline: Given An existing customer of \u0026#34;\u0026lt;customer\u0026gt;\u0026#34;. And A product catalog of \u0026#34;\u0026lt;product-catalog\u0026gt;\u0026#34;. When Order \u0026#34;\u0026lt;order\u0026gt;\u0026#34; is placed. Then The Sales Team are notified of Order \u0026#34;\u0026lt;summary\u0026gt;\u0026#34;. Scenarios: | customer| product-catalog| order| summary||mary-jones|lemons-and-bananas|marys-order|marys-order-summary||john-smith|apples-and-pears|johns-order|johns-order-summary| To keep the feature file readable, the customer, product catalog, placed order \u0026amp; expected sale summary will be loaded from separate json files. The scenarios above give the names of these files (minus the .json extension). They\u0026rsquo;ll look like this for the first scenario:\nmary-jones.json - we want this customer to be present in the system at the start of the test.\n{ \u0026#34;id\u0026#34;: 392, \u0026#34;name\u0026#34;: \u0026#34;Mary Jones\u0026#34; } lemons-and-bananas.json - we want the following catalog of products to be present in the system at the start of the test.\n[ { \u0026#34;id\u0026#34;: 812, \u0026#34;item\u0026#34;: \u0026#34;Lemons\u0026#34;, \u0026#34;unitPrice\u0026#34;: 0.66 }, { \u0026#34;id\u0026#34;: 944, \u0026#34;item\u0026#34;: \u0026#34;Bananas\u0026#34;, \u0026#34;unitPrice\u0026#34;: 0.19 } ] marys-order.json - we want this order to be injected into the system as a placed order.\n{ \u0026#34;customerId\u0026#34;: 392, \u0026#34;items\u0026#34;: [ { \u0026#34;productId\u0026#34;: 812, \u0026#34;quantity\u0026#34;: 2 }, { \u0026#34;productId\u0026#34;: 944, \u0026#34;quantity\u0026#34;: 7 } ] } marys-order-summary.json - given the above preconditions \u0026amp; the injection of our order, we expect the system to produce the following sale summary.\n{ \u0026#34;customerName\u0026#34;: \u0026#34;Mary Jones\u0026#34;, \u0026#34;itemsDescriptions\u0026#34;: [\u0026#34;Lemons\u0026#34;,\u0026#34;Bananas\u0026#34;], \u0026#34;value\u0026#34;: 2.65 } step definitions We\u0026rsquo;ll define the following step definitions to support our feature:\nWe need two @Given steps to set the system up for our test - one for adding customers \u0026amp; one for adding a catalog of the products sold. The actual loading to the database is done in our Pipeleine class which we\u0026rsquo;ll look at later.\n@Given(\u0026#34;An existing customer of {string}.\u0026#34;) public void anExistingCustomer(String customer) throws SQLException { LOG.info(\u0026#34;Loading existing customer {}\u0026#34;, customer); Pipeline.insertCustomer( Utils.loadJson(String.format(\u0026#34;/test-data/customers/%s.json\u0026#34;, customer)) ); } @Given(\u0026#34;A product catalog of {string}.\u0026#34;) public void aProductCatalogOf(String catalog) throws SQLException { LOG.info(\u0026#34;Loading product catalog {}\u0026#34;, catalog); Pipeline.insertProducts( Utils.loadJson(String.format(\u0026#34;/test-data/product-catalogs/%s.json\u0026#34;, catalog)) ); } Our @When step will load the order from file and send it to the orders topic. Again the actual work of sending the message is delegated to the Pipeline class.\n@When(\u0026#34;Order {string} is placed.\u0026#34;) public void orderIsReceived(String order) { orderNumber = ThreadLocalRandom.current().nextInt(1000000); LOG.info(\u0026#34;Injecting order {} with order number {}\u0026#34;, order, orderNumber); Pipeline.send( Topic.ORDERS, orderNumber, Utils.loadJson(String.format(\u0026#34;/test-data/orders/%s.json\u0026#34;, order)) ); } Finally we need to define our @Then step to verify that the expected sale-summary is received on the sales topic. The retrieval of the messages from the Kafka topic is also delegated to the Pipeline class. We assert that the topic contains exactly one message with the expected key (the order number) \u0026amp; sale-summary.\n@Then(\u0026#34;The Sales Team are notified of Order {string}.\u0026#34;) public void salesTeamAreNotifiedOfOrder(String orderSummary) { LOG.info(\u0026#34;Verifying that order summary {} is received.\u0026#34;, orderSummary); Awaitility.await().atMost(Duration.ofSeconds(10)).untilAsserted( () -\u0026gt; assertThat(Pipeline.messagesOn(Topic.SALES)) .hasSize(1) .containsOnlyKeys(orderNumber) .containsValue( Utils.loadJson( String.format(\u0026#34;/test-data/sales-summaries/%s.json\u0026#34;, orderSummary) ) ) ); } the pipeline class The real work will be done in the Pipeline class. It will contain the logic for setting the system up with the required data for our test, injecting messages to a topic and retrieving messages for verification. It will also start our application \u0026amp; the infrastructure we need to test our pipeline.\nresponding to lifecycle events Before we dive into the code though, I need to explain something. Starting Docker containers takes a little time so we want to share the containers between our Cucumber scenarios. When writing tests with JUnit Jupiter this is easy using the annotations @Testcontainers and @Container. When the latter is applied to a static field, Testcontainers will start containers \u0026amp; share them amongst our @Test methods.\nWe need to achieve the same thing with Cucumber. It has its own @Before annotation triggered before each scenario but not yet a @BeforeAll which would be useful for starting containers exactly once.3\nI looked at a couple of options to overcome this but settled on a solution that used Cucumber\u0026rsquo;s Plugin mechanism. This allows us to listen to Cucumber lifecycle events and start our containers exactly once. Our Pipeline class is therefore implemented as a plugin - specifically it implements EventListener and registers handlers so that our pipeline is notified when a test-run start \u0026amp; stops \u0026amp; when a test-case starts.\npublic class Pipeline implements EventListener { ... @Override public void setEventPublisher(EventPublisher publisher) { publisher.registerHandlerFor(TestRunStarted.class, startUp); publisher.registerHandlerFor(TestCaseStarted.class, startTestCase); publisher.registerHandlerFor(TestRunFinished.class, shutDown); } ... } Before looking at the three handlers, let\u0026rsquo;s look at the definition of the containers we\u0026rsquo;ll use. There are a few things worth noting:\n We are attaching a log consumer to each container so that we can see the container\u0026rsquo;s logging in our main log. We are using a Testcontainers wait strategy to wait for a log statement that indicates that the containers are ready for use before the tests run. This is not necessary for our database container as it this is built into the JdbcDatabaseContainer base class. We specify an initialisation script for our database container that creates our customers \u0026amp; products schema.  private static final KafkaContainer KAFKA = new KafkaContainer(DockerImageName.parse(\u0026#34;confluentinc/cp-kafka:6.0.1\u0026#34;)) .withLogConsumer(logConsumer(\u0026#34;kafka-container\u0026#34;)) .waitingFor(Wait.forLogMessage(\u0026#34;.*KafkaServer.*started.*\u0026#34;, 1)); private static final MariaDBContainer\u0026lt;?\u0026gt; DATABASE = new MariaDBContainer\u0026lt;\u0026gt;(DockerImageName.parse(\u0026#34;mariadb:10.5.9\u0026#34;)) .withInitScript(\u0026#34;schema.sql\u0026#34;) .withLogConsumer(logConsumer(\u0026#34;maria-db\u0026#34;)); private static final GenericContainer\u0026lt;?\u0026gt; APPLICATION = new GenericContainer\u0026lt;\u0026gt;(DockerImageName.parse(\u0026#34;order-enrichment-stream\u0026#34;)) .withLogConsumer(logConsumer(\u0026#34;order-enrichment-stream\u0026#34;)) .waitingFor(Wait.forLogMessage(\u0026#34;.*Started Application.*\u0026#34;, 1)); Next we\u0026rsquo;ll look at the three handlers in turn starting with the startUp handler.\nstart-up handler At the start of the test-run, we\u0026rsquo;ll create our three containers \u0026amp; place them on the same docker network, create the required Kafka topics \u0026amp; initialise our Kafka consumer for later message verification. Environment variables are used to configure our spring-boot application using Testcontainers' withEnv().\nprivate final EventHandler\u0026lt;TestRunStarted\u0026gt; startUp = testRunStarted -\u0026gt; { Network network = Network.newNetwork(); KAFKA.withNetwork(network).start(); DATABASE.withNetwork(network).start(); APPLICATION.withNetwork(network) .withEnv(applicationEnvironment()) .start(); Utils.createTopics(KAFKA); kafkaConsumer = Utils.consumerFor(Topic.SALES, KAFKA); }; The environment variables we\u0026rsquo;ll pass to the application container are shown below and include the kafka \u0026amp; database urls. Note that the host part of the url is obtained by getting the network alias of the corresponding container.\nprivate Map\u0026lt;String, String\u0026gt; applicationEnvironment() { return Map.of( \u0026#34;spring.kafka.bootstrapServers\u0026#34;, KAFKA.getNetworkAliases().get(0) + \u0026#34;:9092\u0026#34;, \u0026#34;spring.cloud.stream.kafka.binder.brokers\u0026#34;, KAFKA.getNetworkAliases().get(0) + \u0026#34;:9092\u0026#34;, \u0026#34;spring.cloud.stream.bindings.enrichOrder-in-0.destination\u0026#34;, Topic.ORDERS.value(), \u0026#34;spring.cloud.stream.bindings.enrichOrder-out-0.destination\u0026#34;, Topic.SALES.value(), \u0026#34;spring.datasource.url\u0026#34;, \u0026#34;jdbc:mariadb://\u0026#34; + DATABASE.getNetworkAliases().get(0) + \u0026#34;:3306/\u0026#34; + DATABASE.getDatabaseName(), \u0026#34;spring.datasource.username\u0026#34;, DATABASE.getUsername(), \u0026#34;spring.datasource.password\u0026#34;, DATABASE.getPassword() ); } start-test-case handler The startTestCase handler will run before each scenario and will delete the customers \u0026amp; products from our test database so that it is ready for the next scenario.\nprivate final EventHandler\u0026lt;TestCaseStarted\u0026gt; startTestCase = testCaseStarted -\u0026gt; { Utils.jdbcTemplateFor(DATABASE).update(\u0026#34;delete from customers\u0026#34;); Utils.jdbcTemplateFor(DATABASE).update(\u0026#34;delete from products\u0026#34;); }; shut-down handler Finally, the shutDown handler will stop our containers.\nprivate final EventHandler\u0026lt;TestRunFinished\u0026gt; shutDown = testRunFinished -\u0026gt; { KAFKA.stop(); DATABASE.stop(); APPLICATION.stop(); }; creating database records Our steps require that we are able to add customers \u0026amp; a product catalog to our database before we test a scenario. This will be supported by the following methods.\npublic static void insertCustomer(JsonNode customer) throws SQLException { Utils.jdbcTemplateFor(DATABASE).update( \u0026#34;insert into customers (id, name) values (?, ?)\u0026#34;, customer.get(\u0026#34;id\u0026#34;).intValue(), customer.get(\u0026#34;name\u0026#34;).textValue() ); } public static void insertProducts(JsonNode productCatalog) throws SQLException { JdbcTemplate jdbcTemplate = Utils.jdbcTemplateFor(DATABASE); for (JsonNode eachProduct : productCatalog) { jdbcTemplate.update( \u0026#34;insert into products (id, description, unit_price) values (?, ?, ?)\u0026#34;, eachProduct.get(\u0026#34;id\u0026#34;).intValue(), eachProduct.get(\u0026#34;item\u0026#34;).textValue(), eachProduct.get(\u0026#34;unitPrice\u0026#34;).decimalValue() ); } } sending and receiving messages The final part of our Pipeline class will be supporting the step definitions that involve sending an order message to a kafka topic and checking the sale-summary message on another topic. They are supported by the following methods.\npublic static void send(Topic topic, int orderNumber, JsonNode document) { Utils.send( KAFKA, new ProducerRecord\u0026lt;\u0026gt;( topic.value(), orderNumber, document.toString() ) ); } public static Map\u0026lt;Integer, JsonNode\u0026gt; messagesOn(Topic topic) { return StreamSupport.stream( KafkaTestUtils.getRecords(kafkaConsumer) .records(topic.value()).spliterator(), false ).collect(Collectors.toMap(ConsumerRecord::key, r -\u0026gt; Utils.toJson(r.value()))); } maven \u0026amp; cli configuration  To wrap-up we\u0026rsquo;ll look at what is involved in getting the tests to run from an IDE and maven.\nRunning tests from IntelliJ or Eclipse involves using Cucumber\u0026rsquo;s CLI. This is configured using the cucumber.properties file. We\u0026rsquo;ll use Surefire to run the tests from our Maven build which is instead configured with junit-platform.properties.4 These two files are therefore identical \u0026amp; contain a single property , cucumber.plugin=dev.codecrumbs.sales.test.Pipeline which will ensure Cucumber uses our Pipeline plugin class.\nFor feature files to be found by the maven surefire plugin we also need a class annotated with @Cucumber in the same package as the features. In our case this will be named IntegrationTests.\nThat\u0026rsquo;s it! The tests will now run as part of the maven build or from an IDE. They look like this in IntelliJ:\n source.\nsystem under test.\ncucumber.\ncucumber-jvm.\ntestcontainers.\n  I am showing annotation-based step definitions here as I personally find them more readable than the lambda alternatives available as part of cucumber-java8. \u0026#x21a9;\u0026#xfe0e;\n I initially tried to start the applications under test in the same JVM as the tests \u0026amp; imported the applications via maven dependencies but I had to abandon this approach as I did not want to restrict the applications to having the same versions of common libraries. \u0026#x21a9;\u0026#xfe0e;\n A @BeforeAll annotation will be added in cucumber-jvm v7 - see this issue. \u0026#x21a9;\u0026#xfe0e;\n There may be other approaches that would use the CLI \u0026amp;, therefore, the same configuration file. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://www.codecrumbs.dev/posts/cucumber-integration-tests/","summary":"introduction I\u0026rsquo;ve written a few integration test frameworks where the tests were written as JUnit @Test methods and defined via a fluent API that hid the plumbing from the test author. These were always executed against a mixture of locally installed \u0026amp; in-memory infrastructure.\nThis worked well-enough in small development teams where the developers wrote the tests but wasn\u0026rsquo;t so good when I considered it for a larger project with a bigger team.","title":"integration testing bdd-style"},{"content":"I have yet to work on a project where we have attempted to test the log statements in the software \u0026amp; I\u0026rsquo;ve recently found myself asking why.\nLog statements added by different developers (sometimes guided by a woolly statement on logging in the project coding guidelines) vary in their number, style \u0026amp; usefulness. Logging ends up inconsistent in its level of detail, format \u0026amp; log level.\nThe logging of the same event could end up like any of the following:\n1: Saving order... 2: Creating order for customer [123] for [5] items with value [£56.20]... 3: Adding order [ customer: { ... // full details of customer }, items: { ... // full details of all items }, ... ] In this environment, diagnosing the cause of production issues frequently involves tweaking the log level for certain packages in the hope that the application gives up its secrets and deploying patches with additional log statements when it doesn\u0026rsquo;t.\nIt seems clear that we do care about logging \u0026amp; it is an important part of what we deliver.\nIf we are using AOP to consistently apply logging at the boundary to certain layers and have no other logging, that\u0026rsquo;s great. Testing those log statements is probably less valuable. If not, we need a solution.\nLogCaptor LogCaptor is a simple library that allows us to capture logging from the system under test and make assertions about it. It looks like this:\nclass WidgetServiceTest { private final LogCaptor logCaptor = LogCaptor.forClass(WidgetService.class); ... @Test void testAdd() { String name = \u0026#34;My Widget\u0026#34;; service.add(Widget.of(name)); assertThat(logCaptor.getDebugLogs()) .containsExactly( String.format(\u0026#34;Adding widget with name [%s]\u0026#34;, name) ); } } (this example is also using the awesome AssertJ)\nThe LogCaptor class provides methods for getting the logged messages as a list of LogEvent or strings. For the latter, these can be restricted to a particular logging level.\nThe full LogEvent is useful for more complex assertions. It provides access to arguments and any Throwable as well as the message \u0026amp; log level. The following example shows a test of a log statement that includes an exception.\n@Test void testRemoveWithIllegalId() { int illegalId = -1; assertThatIllegalArgumentException() .isThrownBy(() -\u0026gt; service.remove(illegalId)); List\u0026lt;LogEvent\u0026gt; logEvents = logCaptor.getLogEvents(); assertThat(logEvents) .hasOnlyOneElementSatisfying( event -\u0026gt; { assertThat(event.getMessage()) .isEqualTo(\u0026#34;Error removing widget\u0026#34;); assertThat(event.getLevel()) .isEqualTo(\u0026#34;ERROR\u0026#34;); assertThat(event.getThrowable()) .isPresent() .get(as(InstanceOfAssertFactories.THROWABLE)) .hasMessage( String.format( \u0026#34;Widget id must be greater than 0 but was [%s]\u0026#34;, illegalId ) ) .isInstanceOf(IllegalArgumentException.class); } ); } It works with all common logging libraries and with the Lombok logging annotations.\nThat\u0026rsquo;s LogCaptor.\nlogcaptor.\nassertj.\nsource.\n","permalink":"https://www.codecrumbs.dev/posts/testing-logging/","summary":"I have yet to work on a project where we have attempted to test the log statements in the software \u0026amp; I\u0026rsquo;ve recently found myself asking why.\nLog statements added by different developers (sometimes guided by a woolly statement on logging in the project coding guidelines) vary in their number, style \u0026amp; usefulness. Logging ends up inconsistent in its level of detail, format \u0026amp; log level.\nThe logging of the same event could end up like any of the following:","title":"test your logging"},{"content":"This is my blog, codecrumbs. I\u0026rsquo;m a software developer specialising in Java with a particular interest in developer testing. I started blogging mainly to help me clarify my own thoughts and perhaps remember them beyond the end of the week.\nIn the unlikely event that it benefits someone else, that\u0026rsquo;s great.\n","permalink":"https://www.codecrumbs.dev/about/","summary":"This is my blog, codecrumbs. I\u0026rsquo;m a software developer specialising in Java with a particular interest in developer testing. I started blogging mainly to help me clarify my own thoughts and perhaps remember them beyond the end of the week.\nIn the unlikely event that it benefits someone else, that\u0026rsquo;s great.","title":"about"},{"content":"","permalink":"https://www.codecrumbs.dev/search/","summary":"search","title":"search"}]